# BAYES' THEOREM WITH BINOMIAL DISTRIBUTION

### I. Probability Distribution
A collection of statements that are _exclusive_ and _exhaustive_
- __exclusive__:  given complete information, no more than one of the statements can be true
- __exhaustive__: given complete information, at least one of the statements must be true  

<br>

### II. Joint Probability (_of independent evenets_)
Measures the probability of two (or more) events occurring **simultaneously**.
$$ P(A \cap B) = P(A, B) = P(A)P(B) $$

<br>

### III. Union Probability (_of independent evenets_)
Measures the probability that **at least one** of the events occurs.
$$ P(A \cup B) = P(A) + P(B) - P(A)P(B)$$

<br>

### IV. Marginal Probability (_the Sum Rule_)
Measures the probability of a single event occurring, **regardless of the other event**.<br>
__Sum rule__: The marginal probability is equal to the sum of the joint probabilities.

$$P(A) = P(A,B_1)+...+P(A,B_i) = \sum_B P(A, B)$$

<br>

### V. Conditional Probability (_the Product rule_)
Measures the probability that a statement is true given that some other statement is **true with certainty**<br>
__Product rule__: Conditional probability of A given that B is true is equal to the joint probability that A and B are true, divided by the probability that B is true
$$P(A \mid B) = \frac{P(A, B)}{P(B)}$$

<br>

### VI. Total Probability
Total probability is a form of __marginal probability__, expressed using the __product rule__. It rewrites JOINT probabilities in terms of CONDITIONAL probabilities, weighted by the probabilities of respective scenarios. The total probability of B is a sum of probabilities to achive it by any scenario possible. 
$$P(B) = \sum_i P(B \mid A_i) P(A_i)$$

<br>

### VI. Independence
$$P(A \mid B) = {P(A)}$$

<br>

## __Bayes' Theorem__
__Derivation from the product rule:__

$$P(A \mid B) = \frac{P(A, B)}{P(B)}$$
$$P(A \mid B) P(B) = P(A, B)$$
$$P(A \mid B) P(B) = P(B, A)$$
$$P(A \mid B) P(B) = P(B \mid A) P(A)$$

__Bayes' Theorem:__
$$P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}$$


<br>

## __Inverse Probability__
Refers to the process of determining the probability of a cause given an observed effect. This concept is central to Bayesian probability, where we update our beliefs based on new evidence.

__Technical vocabulary__:

$$
\underbrace{P(\theta_i \mid D)}_{\text{posterior probability}} = 
\frac{\underbrace{P(D \mid \theta_i)}_{\text{likelihood}} 
\underbrace{P(\theta_i)}_{\text{prior probability}}}
{\underbrace{P(D)}_{\text{total probability}}}
$$

__Definitions__:
- *posterior probability* — probability after new data is observed.
- *prior probability* — probability before any data is observed or before new data is observed.
- *likelihood* — standard forward probability of data given parameters.
- *total probability* — probability of the data.


__Explanation of terms on a use case__:
<br>

```plaintext
There are two urns with marbles: 
- The probability of picking a white marble from urn_1 is 0.1.
- The probability of picking a white marble from urn_2 is 0.2.
- We observe three white marbles in a row drawn with replacement. 
- What is the probability that we are observing urn_1? urn_2?
```
The formula:
$$P(\theta_i \mid B) = \frac{P(B \mid \theta_i) P(\theta_i)}{P(B)}$$

Where:
- ${P(\theta_i)}$ - _prior probability_ of dealing with a respective urn. From indifference principle we set it to <br> 
$[0.5, 0.5]$

- ${P(B \mid \theta_i)}$ - _likelihood_ of picking 3 white marbles in a row, given that we dill with a respective urn. It is given (or can be easily calculated) <br>
$[0.1^3, 0.2^3] = [0.001, 0.008]$

- ${P(B)}$ - _total probability_ of picking 3 white marbles in a row (__regardless of the urns__). This is the probability of picking 3 white marbles from each urn, weighted by the probability of dilling with each urn:<br>
$P(B \mid \theta_1)P(\theta_1) + P(B \mid \theta_2)P(\theta_2) = [0.001 * 0.5 + 0.008 * 0.5] = 0.0045 $ 

- ${P(\theta_i \mid B)}$ - _posterior probability_ of observing a respective urn, given 3 white marbles in a row. This is what we want to find:<br>
    > The probability of observing three white marbles coming from urn_1: $P(\theta_1 \mid B) = \frac{(0.001 * 0.5)}{0.0045} = \frac{1}{9}$ <br>
    > The probability of observing three white marbles coming from urn_2: $P(\theta_2 \mid B) = \frac{(0.008 * 0.5)}{0.0045} = \frac{8}{9}$<br>

<br>

## __Updating with New Data__
```plaintext
New information: We draw a fourth marble that is also white.
What is the probability that we are observing urn_1? urn_2?
```

Where:
- ${P(\theta_i)}$ - _prior probability_ of dealing with a respective urn. _Now it equals to the result from the previous step_: <br> 
$[1/9, 8/9]$

- ${P(B \mid \theta_i)}$ - _likelihood_ of picking __1 white marbles__, given that we dill with a respective urn. <br>
$[0.1, 0.2]$

- ${P(B)}$ - _total probability_ of picking __3 white marbles and another one__ in a row (__regardless of the urns__):<br>
$\left( 0.1 \times \frac{1}{9} \right) + \left( 0.2 \times \frac{8}{9} \right) \approx 0.1889$ 

- ${P(\theta_i \mid B)}$ - _posterior probability_ of observing a respective urn, given __3 white marbles and another one__ in a row:<br>
    > The probability of observing it from urn_1: $P(\theta_1 \mid B) = \frac{0.1 * 1/9}{0.1889} = 5.88$% <br>
    > The probability of observing it from urn_2: $P(\theta_2 \mid B) = \frac{(0.2 * 8/9)}{0.1889} = 94.12$% <br>


