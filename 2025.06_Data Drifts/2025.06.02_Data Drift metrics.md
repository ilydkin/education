#  Data Drift Metrics
## I. PSI

$$
\text{PSI} = \sum_{i=1}^{n} \left( \text{Expected}_i - \text{Actual}_i \right) \cdot \ln \left( \frac{\text{Expected}_i}{\text{Actual}_i} \right)
$$

$$
\text{PSI} \in [0,\ \infty)
$$

where:
- $Expected_i$ — mean in i-bin of train data
- $Actual_i$ — mean in i-bin of current data
- $n$ — number of bins

Logic:
- quantifies the distribution shift between two vectors by comparing their proportions across predefined bins
- It is calculated using a logarithmic formula that emphasizes both magnitude and direction of change.
- Binning is required, and the quality of the metric can depend on the number and boundaries of the bins.
- PSI is particularly sensitive to small proportions

Application in Evidently:
 - numerical and categorical

## II. Wasserstein distance (Earth Mover's Distance)

$$
W_1^{\text{norm}}(P, Q) = \frac{1}{b - a} \int_0^1 \left| F_P^{-1}(u) - F_Q^{-1}(u) \right| \, du
$$

$$
W_1^{\text{norm}}(P, Q) \in [0,\ 1]
$$

where:
- $F_P^{-1}(u)$ — quantile function (inverse CDF) of the training dataset  
- $F_Q^{-1}(u)$ — quantile function (inverse CDF) of the current dataset  
- $u \in [0, 1]$ — probability level  
- $[a, b]$ — support range: from min to max across both distributions  
- $W_1^{\text{norm}}(P, Q)$ — normalized Wasserstein distance representing the average shift in quantiles, scaled by the support range

Logic:
- Measures the geometric distance between quantile functions (inverse CDFs) of two distributions
- The normalized version divides the Wasserstein distance by the scale
- Unlike PSI it does not require binning — it operates directly on continuous distributions via their CDFs.

Application in Evidently:
 - only numerical
 - default method for numerical data, if > 1000 objects


## III. Kolmogorov–Smirnov Test
$$
D_{n,m} = \sup_x \left| F_n(x) - F_m(x) \right|
$$

$$
D_{n,m} \in [0,\ 1]
$$

where:
- $F_n(x)$ — empirical cumulative distribution function (ECDF) the training dataset 
- $F_m(x)$ — ECDF of the **current** dataset (e.g., production data)  
- $x$ — any value within the combined domain of both datasets
- $D_{n,m}$ — the maximum vertical distance between the two ECDFs

Logic:
- Measures the max distance between Empirical CDFs of two distributions
- Comes with p-value that defines the drift score (=< 0.05 means drift)

Application in Evidently:
 - numerical (n_unique > 5) for <= 1000 objects


## IV. Jensen–Shannon distance
$$ 
D_{\mathrm{JS}}(P \parallel Q) = \frac{1}{2} D_{\mathrm{KL}}(P \parallel M) + \frac{1}{2} D_{\mathrm{KL}}(Q \parallel M) 
$$ 
where: 
$$ 
M = \frac{1}{2}(P + Q) 
$$ 
and 
$$ 
D_{\mathrm{KL}}(P \parallel Q) = \sum_i P(i) \log_2 \left( \frac{P(i)}{Q(i)} \right) 
$$ 

$$ 
\text{Jensen–Shannon distance} = \sqrt{D_{\mathrm{JS}}(P \parallel Q)} 
$$ 