#  Data Drift Metrics
## I. PSI Population Stability Index
$$
\text{PSI} = \sum_{i=1}^{n} \left( \text{Train}_i - \text{Current}_i \right) \cdot \ln \left( \frac{\text{Train}_i}{\text{Current}_i} \right)
$$

$$
\text{PSI} \in [0,\ \infty)
$$

where:
- $Train_i$ — mean in i-bin of train data
- $Current_i$ — mean in i-bin of current data
- $n$ — number of bins

Logic:
- Quantifies the distribution shift between two vectors by comparing their proportions across predefined bins
- It is calculated using a logarithmic formula that emphasizes both magnitude and direction of change.
- Binning is required, and the quality of the metric can depend on the number and boundaries of the bins.
- PSI is particularly sensitive to small proportions

Application in Evidently:
 - numerical and categorical

## II. Wasserstein distance (Earth Mover's Distance)

$$
W_1^{\text{norm}}(T, C) = \frac{1}{b - a} \int_0^1 \left| F_T^{-1}(u) - F_C^{-1}(u) \right| \, du
$$

$$
W_1^{\text{norm}}(T, C) \in [0,\ 1]
$$

where:
- $F_T^{-1}(u)$ — quantile function (inverse CDF) of the TRAINING dataset  
- $F_Q^{-1}(u)$ — quantile function (inverse CDF) of the CURRENT dataset  
- $u \in [0, 1]$ — probability level  
- $[a, b]$ — support range: from min to max across both distributions  
- $W_1^{\text{norm}}(P, Q)$ — normalized Wasserstein distance representing the average shift in quantiles, scaled by the support range

Logic:
- Measures the geometric distance between quantile functions (inverse CDFs) of two distributions
- The normalized version divides the Wasserstein distance by the scale
- Unlike PSI it does not require binning — it operates directly on continuous distributions via their CDFs.

Application in Evidently:
 - only numerical
 - default method for numerical data, if > 1000 objects


## III. Kolmogorov–Smirnov Test
$$
D_{T, C} = \sup_x \left| F_T(x) - F_ C(x) \right|
$$

$$
D_{T, C} \in [0,\ 1]
$$

where:
- $F_T(x)$ — empirical cumulative distribution function (ECDF) of the TRAINING dataset 
- $F_C(x)$ — ECDF of the CURRENT dataset   
- $D_{T, C}$ — the maximum vertical distance between the two ECDFs

Logic:
- Measures the max distance between Empirical CDFs of two distributions
- Comes with p-value that defines the drift score (=< 0.05 means drift)

Application in Evidently:
 - numerical (n_unique > 5) for <= 1000 objects

## IV. Jensen–Shannon distance
1. **Entropy**: 
    $$ H(p) = -\sum_i p_ilog_2(p_i) $$
    
    where:
    - $p$ is a probability of event $i$
    
    **intuition**: 
    - entropy is the expected value of information
#

2. **Cross Entropy**:
    $$ H(p, q) = -\sum_i p_ilog_2(q_i) $$
    
    where:
    - $p$ is the true distribution  
    - $q$ is the estimated (model) distribution  
    
    **intuition**
    - How many bits are needed to encode events from $p$ using $q$ 
#

3. **KLD Kullback–Leibler Divergence**:
    $$ D_{\mathrm{KL}}(p \parallel q) = \sum_i p_ilog_2(\frac{p_i}{q_i})$$
    $$ = \sum_i p_ilog_2(p_i) -\sum_i p_ilog_2(q_i)$$
    $$ = H(p,q) - H(p)$$
    
    **intuition**:
    - Kullback–Leibler Divergence measures the difference between Cross Entropy and Entropy. 
    - It equals zero only when the two distributions are identical, and is strictly positive otherwise.
#

4. **Mixture Distribution**:
    $$ \vec{m} = \frac{\vec{p} + \vec{q}}{2} $$
    
    **intuition**:
    - $\vec{m}$ is the average (mixture) distribution  
    - acts as a symmetric reference between $\vec{p}$ and $\vec{q}$ 
#
5. **Jensen–Shannon Divergence**:
    $$D_{\mathrm{JS}}(\vec{p} \parallel \vec{q}) = \frac{D_{\mathrm{KL}}(\vec{p} \parallel \vec{m}) + D_{\mathrm{KL}}(\vec{q} \parallel \vec{m})}{2}$$
    where:
    - $\vec{m}$ is the mixture distribution  
    
    **intuition**: 
    - symmetric, smoothed version of KL Divergence 
    - avoids infinite values and handles zero probabilities gracefully
#
6. **Jensen–Shannon Distance**:
    $$ \text{Jensen–Shannon distance} = \sqrt{D_{\mathrm{JS}}(\vec{p} \parallel \vec{q})} $$

    **Properties**: 
    - The metric lies within the range [0, 1] and is symmetric
    
    **Intuition**: 
    - When calculating KLD, we took base-2 logarithms of probabilities in [0, 1], which is similar to squaring.
    - Taking the square root brings the values back into the [0, 1] range
