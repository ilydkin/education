#  Data Drift Metrics
## I. PSI

$$
\text{PSI} = \sum_{i=1}^{n} \left( \text{Expected}_i - \text{Actual}_i \right) \cdot \ln \left( \frac{\text{Expected}_i}{\text{Actual}_i} \right)
$$

where:
- $Expected_i$ — mean in i-bin of train data
- $Actual_i$ — mean in i-bin of current data
- $n$ — number of bins

Logic:
- quantifies the distribution shift between two vectors by comparing their proportions across predefined bins
- It is calculated using a logarithmic formula that emphasizes both magnitude and direction of change.
- Binning is required, and the quality of the metric can depend on the number and boundaries of the bins.
- PSI is particularly sensitive to small proportions

Application in Evidently:
 - tabular data numerical and categorical



## II. Wasserstein distance (normed)

$$
W_1^{\text{norm}}(P, Q) = \frac{1}{b - a} \int_0^1 \left| F_P^{-1}(u) - F_Q^{-1}(u) \right| \, du
$$

where:
- $F_P^{-1}(u)$ — quantile function (inverse CDF) of the training dataset  
- $F_Q^{-1}(u)$ — quantile function (inverse CDF) of the current dataset  
- $u \in [0, 1]$ — probability level  
- $[a, b]$ — support range: from min to max across both distributions  
- $W_1^{\text{norm}}(P, Q)$ — normalized Wasserstein distance representing the average shift in quantiles, scaled by the support range

Logic:
- Measures the geometric distance between quantile functions (inverse CDFs) of two distributions
- The normalized version divides the Wasserstein distance by the scale
- Unlike PSI it does not require binning — it operates directly on continuous distributions via their CDFs.

Application in Evidently:
 - only numerical
 - default method for numerical data, if > 1000 objects

